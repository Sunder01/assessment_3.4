Hdfs federation and high availability
The existing architecture hdfs architecture is improved by hdfs federation through a clear separation of namespace and storage enabling generic block storage layer. It supports multiple namespace in the cluster to improve scalablity and isolation. It increases the usability and implementation of hdfs cluster.
The name service is scaled horizantlly in hadoop federation. It uses several name nodes or namspaces which are independent of each other. These name nodes does not need interconnection. These data nodes are used as commen storage by all the namenodes. The name nodes contains information of all the data nodes. These data nodes responds to te commands to all the name nodes. A block pool is a set of blocks that belong to a single name space. IN a cluster the data nodes stores blocks for all the block pools. These block pools are managed independently. This enables the name space to generate blocks id for new block wthout informinf other namespaces. If one of the name node fails the data nodes can be accessed from other name nodes .
Benefits of hadoop federation
Hadoop federation comes up with some advantages and benefits which are listed as under –
Scalability and Isolation – Multiple namenodes horizontally scales up in the file system namespace. This actually separates namespace volumes for users and categories of application and provides an absolute isolation.Generic Storage Service – The block level pool abstraction allows the architecture to build new file systems on top of block storage. We can easily build new applications on the block storage layer without using the file system interface. Customized categories of block pool can also be built which are different from the default block pool.Simple Design – Namenodes and namespaces are independent of each other. There is hardly any scenario which requires changing the existing name nodes. Each name node is built to be robust. Federation is also backward compatible. It easily integrates with the existing single node deployments which work without any configuration changes.

Hdfs handling write failure
The data is written in sequential blocks when data moved from client to hdfs. The hdfs breaks the block into packets and propagates them to the datanodes in the write pipeline.
The stages of writing data are pipe line setup,data straming and close. If the pipeline fails Recovery from Pipeline Setup FailureIf the pipeline was created for a new block, the client abandons the block and asks the NameNode for a new block and a new list of DataNodes. The pipeline is reinitialized for the new block.If the pipeline was created to append to a block, the client rebuilds the pipeline with the remaining DataNodes and increments the block’s generation stamp.
Recovery from Data Streaming Failure
When a DataNode in the pipeline detects an error (for example, a checksum error or a failure to write to disk), that DataNode takes itself out of the pipeline by closing up all TCP/IP connections. If the data is deemed not corrupted, it also writes buffered data to the relevant block and checksum files.When the client detects the failure, it stops sending data to the pipeline, and reconstructs a new pipeline using the remaining good DataNodes. As a result, all replicas of the block are bumped up to a new GS.The client resumes sending data packets with this new GS. If the data sent has already been received by some of the DataNodes, they just ignore the packet and pass it downstream in the pipeline.
Recovery from Close Failure
When the client detects a failure in the close state, it rebuilds the pipeline with the remaining DataNodes. Each DataNode bumps up the block’s GS and finalizes the replica if it’s not finalized yet.

